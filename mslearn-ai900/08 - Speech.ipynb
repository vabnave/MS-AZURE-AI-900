{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech\n",
    "\n",
    "Increasingly, we expect to be able to communicate with artificial intelligence (AI) systems by talking to them, often with the expectation of a spoken response.\n",
    "\n",
    "![A robot speaking](./images/speech.jpg)\n",
    "\n",
    "*Speech recognition* (an AI system interpreting spoken language) and *speech synthesis* (an AI system generating a spoken response) are the key components of a speech-enabled AI solution.\n",
    "\n",
    "## Create a Cognitive Services resource\n",
    "\n",
    "To build software that can interpret audible speech and respond verbally, you can use the **Speech** cognitive service, which provides a simple way to transcribe spoken language into text and vice-versa.\n",
    "\n",
    "If you don't already have one, use the following steps to create a **Cognitive Services** resource in your Azure subscription:\n",
    "\n",
    "> **Note**: If you already have a Cognitive Services resource, just open its **Quick start** page in the Azure portal and copy its key and endpoint to the cell below. Otherwise, follow the steps below to create one.\n",
    "\n",
    "1. In another browser tab, open the Azure portal at https://portal.azure.com, signing in with your Microsoft account.\n",
    "2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n",
    "    - **Subscription**: *Your Azure subscription*.\n",
    "    - **Resource group**: *Select or create a resource group with a unique name*.\n",
    "    - **Region**: *Choose any available region*:\n",
    "    - **Name**: *Enter a unique name*.\n",
    "    - **Pricing tier**: S0\n",
    "    - **I confirm I have read and understood the notices**: Selected.\n",
    "3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Overview** page, click the link to manage the keys for the service. You will need the key and location to connect to your cognitive services resource from client applications.\n",
    "\n",
    "### Get the Key and Location for your Cognitive Services resource\n",
    "\n",
    "To use your cognitive services resource, client applications need its authentication key and location:\n",
    "\n",
    "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
    "2. Copy the **Location** for your resource and and paste it in the code below, replacing **YOUR_COG_LOCATION**.\n",
    ">**Note**: Stay on the **Keys and Endpoint** page and copy the **Location** from this page (example: _westus_). Please _do not_ add spaces between words for the Location field. \n",
    "3. Run the code below by clicking the **Run cell** (&#9655;) button to the left of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1599695240794
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use cognitive services in northeurope using key 2c12ab54af524b968519333205c34b44\n"
     ]
    }
   ],
   "source": [
    "cog_key = '2c12ab54af524b968519333205c34b44'\n",
    "cog_location = 'northeurope'\n",
    "\n",
    "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech recognition\n",
    "\n",
    "Suppose you want to build a home automation system that accepts spoken instructions, such as \"turn the light on\" or \"turn the light off\". Your application needs to be able to take the audio-based input (your spoken instruction), and interpret it by transcribing it to text that it can then parse and analyze.\n",
    "\n",
    "Now you're ready to transcribe some speech. The input can be from a **microphone** or an **audio file**. \n",
    "\n",
    "### Speech Recognition with a microphone\n",
    "\n",
    "Let's try with a microphone input first. Run the cell below and **immediately** say out loud **\"turn the light on\"**. The speech-to-text capabilities of the Speech service will transcribe the audio. The output should be your speech in text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1599695250434
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)\n[CALL STACK BEGIN]\n\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0xb98d) [0x7f48d87a598d]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa526) [0x7f48d982b526]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1faae4) [0x7f48d982bae4]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fe082) [0x7f48d982f082]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa526) [0x7f48d982b526]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa660) [0x7f48d982b660]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa6c1) [0x7f48d982b6c1]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10fc59) [0x7f48d9740c59]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10fc59) [0x7f48d9740c59]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1d3f4e) [0x7f48d9804f4e]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e2d82) [0x7f48d9813d82]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e1a08) [0x7f48d9812a08]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e1cf9) [0x7f48d9812cf9]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x260ad7) [0x7f48d9891ad7]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x109) [0x7f48d98932c2]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/_speech_py_impl.so(+0x106b70) [0x7f48da15db70]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/_speech_py_impl.so(+0x8ceff) [0x7f48da0e3eff]\n[CALL STACK END]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b06dc66bb07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Have students say \"turn the light on\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mspeech_recognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpeechRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Use a one-time, synchronous call to transcribe the speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/speech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_language_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_detect_source_language_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecognize_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSpeechRecognitionResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/speech.py\u001b[0m in \u001b[0;36m_get_impl\u001b[0;34m(reco_type, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msource_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mauto_detect_source_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maudio_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreco_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreco_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maudio_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)\n[CALL STACK BEGIN]\n\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0xb98d) [0x7f48d87a598d]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa526) [0x7f48d982b526]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1faae4) [0x7f48d982bae4]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fe082) [0x7f48d982f082]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa526) [0x7f48d982b526]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa660) [0x7f48d982b660]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1fa6c1) [0x7f48d982b6c1]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10fc59) [0x7f48d9740c59]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10fc59) [0x7f48d9740c59]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1d3f4e) [0x7f48d9804f4e]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e2d82) [0x7f48d9813d82]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e1a08) [0x7f48d9812a08]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e1cf9) [0x7f48d9812cf9]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x260ad7) [0x7f48d9891ad7]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x109) [0x7f48d98932c2]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/_speech_py_impl.so(+0x106b70) [0x7f48da15db70]\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azure/cognitiveservices/speech/_speech_py_impl.so(+0x8ceff) [0x7f48da0e3eff]\n[CALL STACK END]\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import IPython\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
    "\n",
    "\n",
    "# Configure speech recognizer\n",
    "speech_config = SpeechConfig(cog_key, cog_location)\n",
    "\n",
    "# Have students say \"turn the light on\" \n",
    "speech_recognizer = SpeechRecognizer(speech_config)\n",
    "\n",
    "# Use a one-time, synchronous call to transcribe the speech\n",
    "speech = speech_recognizer.recognize_once()\n",
    "\n",
    "print(speech.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (!) Check In\n",
    "\n",
    "Were you able to run the cell and translate your speech to text? If the above cell does not give a text output (example output: _Turn the light on._), try running the cell again and **immediately** say out loud \"turn the light on\".\n",
    "\n",
    "### Speech Recognition with an audio file\n",
    "\n",
    "If the cell above does not give a text output, your microphone may not be set up to accept input. Instead, run the cell below to see the Speech Recognition service in action with an **audio file** instead of **microphone input**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playsound in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.2.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gi (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for gi\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install playsound\n",
    "!pip install gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn the light on.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
    "\n",
    "# Get spoken command from audio file\n",
    "file_name = 'light-on.wav'\n",
    "audio_file = os.path.join('data', 'speech', file_name)\n",
    "\n",
    "# Configure speech recognizer\n",
    "speech_config = SpeechConfig(cog_key, cog_location)\n",
    "audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
    "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "# Use a one-time, synchronous call to transcribe the speech\n",
    "speech = speech_recognizer.recognize_once()\n",
    "\n",
    "# Play audio and show transcribed text\n",
    "playsound(audio_file)\n",
    "print(speech.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech synthesis\n",
    "\n",
    "So now you've seen how the Speech service can be used to transcribe speech into text; but what about the opposite? How can you convert text into speech?\n",
    "\n",
    "Well, let's assume your home automation system has interpreted a command to turn the light on. An appropriate response might be to acknowledge the command verbally (as well as actually performing the commanded task!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599695261170
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "%matplotlib inline\n",
    "\n",
    "# Get text to be spoken\n",
    "response_text = 'Turning the light on.'\n",
    "\n",
    "# Configure speech synthesis\n",
    "speech_config = SpeechConfig(cog_key, cog_location)\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config)\n",
    "\n",
    "# Transcribe text into speech\n",
    "result = speech_synthesizer.speak_text(response_text)\n",
    "\n",
    "# Display an appropriate image \n",
    "file_name = response_text + \"jpg\"\n",
    "img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n",
    "plt.axis('off')\n",
    "plt. imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the **response_text** variable to *Turning the light off.* (including the period at the end) and run the cell again to hear the result.\n",
    "\n",
    "## Learn more\n",
    "\n",
    "You've seen a very simple example of using the Speech cognitive service in this notebook. You can learn more about [speech-to-text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) and [text-to-speech](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) in the Speech service documentation."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
